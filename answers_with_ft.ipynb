{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2efeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n",
    "\n",
    "\n",
    "def create_context(\n",
    "    question, search_file_id, max_len=1800, search_model=\"ada\", max_rerank=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the search file.\n",
    "    :param question: The question\n",
    "    :param search_file_id: The file id of the search file\n",
    "    :param max_len: The maximum length of the returned context (in tokens)\n",
    "    :param search_model: The search model to use\n",
    "    :param max_rerank: The maximum number of reranking\n",
    "    :return: The context\n",
    "    \"\"\"\n",
    "    # TODO: openai.Engine(search_model) is deprecated\n",
    "    results = client.Engine(search_model).search(\n",
    "        search_model=search_model,\n",
    "        query=question,\n",
    "        max_rerank=max_rerank,\n",
    "        file=search_file_id,\n",
    "        return_metadata=True,\n",
    "    )\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "    for result in results[\"data\"]:\n",
    "        cur_len += int(result[\"metadata\"]) + 4\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "        returns.append(result[\"text\"])\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)\n",
    "\n",
    "\n",
    "def answer_question(\n",
    "    search_file_id=\"<SEARCH_FILE_ID>\",\n",
    "    fine_tuned_qa_model=\"<FT_QA_MODEL_ID>\",\n",
    "    question=\"Which country won the European Football championship in 2021?\",\n",
    "    max_len=1800,\n",
    "    search_model=\"ada\",\n",
    "    max_rerank=10,\n",
    "    debug=False,\n",
    "    stop_sequence=[\"\\n\", \".\"],\n",
    "    max_tokens=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the search file, using your fine-tuned model.\n",
    "    :param question: The question\n",
    "    :param fine_tuned_qa_model: The fine tuned QA model\n",
    "    :param search_file_id: The file id of the search file\n",
    "    :param max_len: The maximum length of the returned context (in tokens)\n",
    "    :param search_model: The search model to use\n",
    "    :param max_rerank: The maximum number of reranking\n",
    "    :param debug: Whether to output debug information\n",
    "    :param stop_sequence: The stop sequence for Q&A model\n",
    "    :param max_tokens: The maximum number of tokens to return\n",
    "    :return: The answer\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        search_file_id,\n",
    "        max_len=max_len,\n",
    "        search_model=search_model,\n",
    "        max_rerank=max_rerank,\n",
    "    )\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "    try:\n",
    "        # fine-tuned models requires model parameter, whereas other models require engine parameter\n",
    "        model_param = (\n",
    "            {\"model\": fine_tuned_qa_model}\n",
    "            if \":\" in fine_tuned_qa_model\n",
    "            and fine_tuned_qa_model.split(\":\")[1].startswith(\"ft\")\n",
    "            else {\"engine\": fine_tuned_qa_model}\n",
    "        )\n",
    "        response = client.chat.completions.create(prompt=f\"Answer the question based on the context below\\n\\nText: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\",\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=stop_sequence,\n",
    "        **model_param)\n",
    "        return response[\"choices\"][0][\"text\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16557bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Rudimentary functionality of the answers endpoint with a fine-tuned Q&A model.\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--search_file_id\", help=\"Search file id\", required=True, type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--fine_tuned_qa_model\", help=\"Fine-tuned QA model id\", required=True, type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--question\", help=\"Question to answer\", required=True, type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_len\",\n",
    "        help=\"Maximum length of the returned context (in tokens)\",\n",
    "        default=1800,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--search_model\", help=\"Search model to use\", default=\"ada\", type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_rerank\",\n",
    "        help=\"Maximum number of reranking for the search\",\n",
    "        default=10,\n",
    "        type=int,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--debug\", help=\"Print debug information (context used)\", action=\"store_true\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stop_sequence\",\n",
    "        help=\"Stop sequences for the Q&A model\",\n",
    "        default=[\"\\n\", \".\"],\n",
    "        nargs=\"+\",\n",
    "        type=str,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_tokens\",\n",
    "        help=\"Maximum number of tokens to return\",\n",
    "        default=100,\n",
    "        type=int,\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    response = answer_question(\n",
    "        search_file_id=args.search_file_id,\n",
    "        fine_tuned_qa_model=args.fine_tuned_qa_model,\n",
    "        question=args.question,\n",
    "        max_len=args.max_len,\n",
    "        search_model=args.search_model,\n",
    "        max_rerank=args.max_rerank,\n",
    "        debug=args.debug,\n",
    "        stop_sequence=args.stop_sequence,\n",
    "        max_tokens=args.max_tokens,\n",
    "    )\n",
    "    print(f\"Answer:{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
